---
layout: page
permalink: /learning/
title: Learning
---

<center><img src="../images/brain.png" style="height:200px"></center>

## Neural network theory: learning & generalisation

Neural networks are a powerful tool in modern machine learning, driving progress in areas ranging from [protein folding](https://www.nature.com/articles/s41586-019-1923-7) to [natural language processing](https://arxiv.org/abs/2005.14165). This half of the course will cover theoretical results with a direct bearing on machine learning practice. The course will tackle questions such as:

- How should I wire up my neural network?
- What class of functions does my network realise?
- If my wide 2-layer network can fit any dataset, why try anything else?
- How far is it safe to perturb my neural network during learning?
- Why does my network with more parameters than training data still generalise?
- Why is VC dimension not a relevant complexity measure for my network?
- How much information did my neural network extract from the training data?

*Health warning: these questions are still the subject of active research. This course will present the instructor's best understanding of the issues and their resolutions.*

### Instructor

This part of the course will be taught by [Jeremy Bernstein](https://jeremybernste.in) ([bernstein@caltech.edu](mailto:bernstein@caltech.edu)).