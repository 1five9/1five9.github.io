---
layout: page
permalink: /learning/
title: Learning
---

<center><img src="../images/brain.png" style="height:200px"></center>

## Neural network theory: learning & generalisation

Neural networks are a powerful tool in modern machine learning, driving progress in areas ranging from [protein folding](https://www.nature.com/articles/s41586-019-1923-7) to [natural language processing](https://arxiv.org/abs/2005.14165). This half of the course will cover theoretical results with a direct bearing on machine learning practice. The course will tackle questions such as:

- How should I wire up my neural network?
- What class of functions does my network realise?
- If my wide 2-layer network can fit any dataset, why try anything else?
- How far is it safe to perturb my neural network during learning?
- Why does my network with more parameters than training data still generalise?
- Why is VC dimension not a relevant complexity measure for my network?
- How much information did my neural network extract from the training data?

*Health warning: these questions are still the subject of active research. This course will present the instructor's best understanding of the issues and their resolutions.*

### Instructor

This part of the course will be taught by [Jeremy Bernstein](https://jeremybernste.in) ([bernstein@caltech.edu](mailto:bernstein@caltech.edu)).

### Lecture 7 references

Network topologies:
- [Backpropagation applied to handwritten zip code recognition](https://ieeexplore.ieee.org/document/6795724)—CNNs;
- [Attention is all you need](https://arxiv.org/abs/1706.03762)—transformers.

Neural architecture search:
- [DARTS: differentiable architecture search](https://arxiv.org/abs/1806.09055);
- [Neural architecture search with reinforcement learning](https://arxiv.org/abs/1611.01578).

Local network design:
- [Centered weight normalization](https://ieeexplore.ieee.org/document/8237567)—weight constraints;
- [Batch normalization](https://arxiv.org/abs/1502.03167);
- [Normalization propagation](https://arxiv.org/abs/1603.01431)—nonlinearity design.

Perturbation theory:
- [Perturbation theory for the singular value decomposition](https://users.math.msu.edu/users/iwenmark/Teaching/MTH995/Papers/SVD_Stewart.pdf);
- [Perturbation theory for multilayer perceptrons](https://arxiv.org/abs/2002.03432).

### Lecture 8 references
### Lecture 9 references
### Lecture 10 references
### Lecture 11 references
### Lecture 12 references
